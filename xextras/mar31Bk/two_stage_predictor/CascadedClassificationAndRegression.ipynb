{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# # Load the classification model\n",
    "# classification_model = tf.keras.models.load_model('C:\\\\Users\\\\thiru\\\\Desktop\\\\Aashritha project\\\\python_code\\\\New_24_03_TTC_classification_model.h5')\n",
    "\n",
    "# # Load the regression model\n",
    "# regression_model = tf.keras.models.load_model('C:\\\\Users\\\\thiru\\\\Desktop\\\\Aashritha project\\\\python_code\\\\TTC_regression_model.h5')\n",
    "\n",
    "# # Define the input shape for the classification model\n",
    "# classification_input_shape = [4,]\n",
    "\n",
    "# # Define the input layer for the regression model\n",
    "# regression_input_layer = tf.keras.layers.Input(shape=classification_input_shape[1:])\n",
    "\n",
    "# # Connect the output of the classification model to the input of the regression model\n",
    "# classification_output = classification_model(regression_input_layer)\n",
    "# regression_input = tf.keras.layers.concatenate([regression_input_layer, classification_output])\n",
    "\n",
    "# # Define a lambda layer that conditionally activates the regression model based on the output of the classification model\n",
    "# regression_activation = tf.keras.layers.Lambda(lambda x: x[0] * x[1], name='regression_activation')([regression_input, classification_output])\n",
    "\n",
    "# # Connect the regression model to the output of the classification model, using the regression_activation layer to conditionally activate it\n",
    "# regression_output = regression_activation * regression_model(regression_input)\n",
    "\n",
    "# # Define the combined model that includes both the classification and regression models\n",
    "# combined_model = tf.keras.models.Model(inputs=regression_input_layer, outputs=regression_output)\n",
    "\n",
    "# # Compile the combined model with the desired loss function and optimizer\n",
    "# combined_model.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the input shape for both models\n",
    "input_shape = (4,)\n",
    "\n",
    "# Define the classification model\n",
    "# classification_input = tf.keras.layers.Input(shape=input_shape)\n",
    "# x = tf.keras.layers.Dense(8, activation='relu')(classification_input)\n",
    "# x = tf.keras.layers.Dense(2, activation='softmax', name='classification_output')(x)\n",
    "classification_model = tf.keras.models.load_model('C:\\\\Users\\\\thiru\\\\Desktop\\\\Aashritha project\\\\python_code\\\\New_24_03_TTC_classification_model.h5')\n",
    "\n",
    "# Define the regression model\n",
    "# regression_input = tf.keras.layers.Input(shape=input_shape)\n",
    "# x = tf.keras.layers.Dense(16, activation='relu')(regression_input)\n",
    "# x = tf.keras.layers.Dense(1, name='regression_output')(x)\n",
    "regression_model = tf.keras.models.load_model('C:\\\\Users\\\\thiru\\\\Desktop\\\\Aashritha project\\\\python_code\\\\TTC_regression_model.h5')\n",
    "\n",
    "# Define the combined model that includes both models\n",
    "combined_input = tf.keras.layers.Input(shape=input_shape)\n",
    "classification_output = classification_model(combined_input)\n",
    "classification_decision = tf.argmax(classification_output, axis=1)\n",
    "regression_output = regression_model(combined_input)\n",
    "\n",
    "def conditional_activation(inputs):\n",
    "    classification_output, classification_decision, regression_output = inputs\n",
    "    return tf.where(tf.equal(classification_decision, 1), regression_output, classification_output)\n",
    "\n",
    "final_output = tf.keras.layers.Lambda(conditional_activation, name='final_output')([classification_output, classification_decision, regression_output])\n",
    "combined_model = tf.keras.models.Model(inputs=combined_input, outputs=final_output)\n",
    "\n",
    "# Compile the combined model with the desired loss function and optimizer\n",
    "combined_model.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model.predict(X_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report for training data for non linear model\n",
    "#%%time\n",
    "history = combined_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=1000,\n",
    "    batch_size=64,\n",
    "    verbose=1,\n",
    "# Calculate validation results on 20% of the training data.\n",
    "    validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\thiru\\Desktop\\Aashritha project\\python_code\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Print the current working directory\n",
    "print(\"Current working directory:\", cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv('D:\\\\NewFinalDatasetsForNeuralNetwork\\\\crash_noncrash_combined_csv_09_03_2023__13_24.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.displot(dataset['TTC'], kde = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting TTC from the other columns\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 80-20 Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is only used for plotting the training dataset in the below cell, alongwith the column names\n",
    "joined_train_dataset = X_train\n",
    "train_dataset = pd.DataFrame(joined_train_dataset, columns = ['NORMALIZED PEDESTRIAN POSITION X', 'NORMALIZED PEDESTRIAN POSITION Y', 'NORMALIZED PEDESTRIAN DIRECTION', 'NORMALIZED PEDESTRIAN SPEED'])\n",
    "train_dataset.loc[:,'TTC'] = y_train\n",
    "#print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "# Defining first layer of the neural network, which normalizes the input data on the fly\n",
    "normalizer_layer = keras.layers.Normalization(axis=-1)\n",
    "normalizer_layer = layers.Normalization(input_shape=[4,], axis=None)\n",
    "# Adapting normalizer layer to the input train data shape\n",
    "normalizer_layer.adapt(np.array(X_train))\n",
    "\n",
    "# Defines the model and compilation\n",
    "def build_and_compile_model(normalizer_layer):\n",
    "  nn_model = keras.Sequential([\n",
    "      normalizer_layer,\n",
    "      layers.Dense(64, activation='relu'),\n",
    "      layers.Dense(128, activation='relu'),\n",
    "      layers.Dense(64, activation='relu'),\n",
    "      layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  nn_model.compile(loss='mean_squared_error',\n",
    "                optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "                metrics=['mse'])\n",
    "  return nn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the neural network model and compiling it using the normalization layer adapted to the shape of our input training dataset\n",
    "dnn_model = build_and_compile_model(normalizer_layer)\n",
    "dnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report for training data for non linear model\n",
    "#%%time\n",
    "history = dnn_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=1000,\n",
    "    batch_size=64,\n",
    "    verbose=1,\n",
    "# Calculate validation results on 20% of the training data.\n",
    "    validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------------------------------Regression Model----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a new column 'TTC Category' and bin values based on their range\n",
    "bins = [-1, 7, np.inf]\n",
    "labels = ['crash', 'no_crash']\n",
    "dataset['SCENARIO TYPE'] = pd.cut(dataset['TTC'], bins=bins, labels=labels)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Select column to one hot encode\n",
    "col_to_encode = 'SCENARIO TYPE'\n",
    "\n",
    "# Perform one hot encoding\n",
    "one_hot_encoded = pd.get_dummies(dataset[col_to_encode], prefix=col_to_encode)\n",
    "\n",
    "# Append one hot encoded columns to original dataset\n",
    "dataset = pd.concat([dataset, one_hot_encoded], axis=1)\n",
    "\n",
    "# Drop original column that was one hot encoded\n",
    "dataset = dataset.drop(col_to_encode, axis=1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['SCENARIO TYPE'] = data_transformed\n",
    "dataset = dataset.drop('TTC', axis=1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset into features and labels\n",
    "X = dataset.drop(columns=[\"SCENARIO TYPE_crash\", \"SCENARIO TYPE_no_crash\"]).values\n",
    "#X = dataset.drop(columns=[\"SCENARIO TYPE\"]).values\n",
    "#y = dataset[[\"SCENARIO TYPE\"]].values\n",
    "y = dataset[[\"SCENARIO TYPE_crash\", \"SCENARIO TYPE_no_crash\"]].values\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "# Defining first layer of the neural network, which normalizes the input data on the fly\n",
    "normalizer_layer = keras.layers.Normalization(axis=-1)\n",
    "normalizer_layer = keras.layers.Normalization(input_shape=[4,], axis=None)\n",
    "# Adapting normalizer layer to the input train data shape\n",
    "normalizer_layer.adapt(np.array(X_train))\n",
    "\n",
    "# Defines the model and compilation\n",
    "def build_and_compile_model(normalizer_layer):\n",
    "  nn_model = keras.Sequential([\n",
    "      normalizer_layer,\n",
    "      keras.layers.Dense(128, activation='relu', kernel_initializer='he_uniform'),\n",
    "      keras.layers.Dense(64, activation='sigmoid'),\n",
    "      keras.layers.Dense(32, activation='relu'),\n",
    "      keras.layers.Dense(y.shape[1], activation=\"softmax\")\n",
    "  ])\n",
    "\n",
    "  nn_model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=[\"categorical_accuracy\"])\n",
    "  return nn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_model = build_and_compile_model(normalizer_layer)\n",
    "# Train the model\n",
    "history = dnn_model.fit(X_train, y_train, epochs=300, batch_size=128, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = dnn_model.evaluate(X_test, y_test)\n",
    "print(\"Test set accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
